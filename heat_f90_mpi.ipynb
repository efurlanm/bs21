{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heat F90 MPI\n",
    "\n",
    "F90 Parallel (MPI) Implementation of the Test Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting stencil_mpi.f90\n"
     ]
    }
   ],
   "source": [
    "%%writefile stencil_mpi.f90\n",
    "! Revision 2020-08-11\n",
    "! Based on code originally written by C. R. S.\n",
    "\n",
    "program stencil\n",
    "    use MPI\n",
    "    implicit none\n",
    "    integer :: n=0          ! nxn grid\n",
    "    integer :: energy=0     ! energy to be injected per iteration\n",
    "    integer :: niters=0     ! number of iterations\n",
    "    integer :: iters, i, j, px, py, rx, ry\n",
    "    integer :: north, south, west, east, bx, by, offx, offy\n",
    "    integer :: nargs=0, iargc\n",
    "    integer :: mpirank, mpisize, mpitag=1, mpierror\n",
    "    integer, dimension(3) :: args\n",
    "    integer, dimension(2) :: pdims=0\n",
    "    integer, dimension(4) :: sendrequest, recvrequest\n",
    "    double precision :: mpiwtime=0.0, heat=0.0, rheat=0.0\n",
    "    double precision, dimension(:), allocatable   :: sendnorthgz, sendsouthgz\n",
    "    double precision, dimension(:), allocatable   :: recvnorthgz, recvsouthgz\n",
    "    double precision, dimension(:,:), allocatable :: aold, anew\n",
    "    character(len=50)                             :: argv\n",
    "\n",
    "    integer, parameter  :: nsources=3        ! three heat sources\n",
    "    ! locnsources = number of sources in my area\n",
    "    integer             :: locnsources=0, locx, locy\n",
    "    ! locsources = sources local to my rank\n",
    "    integer, dimension(nsources, 2) :: locsources=0, sources\n",
    "\n",
    "    call MPI_Init(mpierror)\n",
    "    call MPI_Comm_rank(MPI_COMM_WORLD, mpirank, mpierror)\n",
    "    call MPI_Comm_size(MPI_COMM_WORLD, mpisize, mpierror)\n",
    "\n",
    "    if (mpirank == 0) then                          ! rank 0 argument checking\n",
    "        mpiwtime = -MPI_Wtime()     ! inicializa contador de tempo\n",
    "        nargs = iargc()\n",
    "        call getarg(1, argv); read(argv, *) n       ! nxn grid\n",
    "        call getarg(2, argv); read(argv, *) energy  ! energy to be injected\n",
    "        call getarg(3, argv); read(argv, *) niters  ! number of iterations\n",
    "        args = [ n, energy, niters ]                ! distribute arguments\n",
    "        call MPI_Bcast(args, 3, MPI_INTEGER, 0, MPI_COMM_WORLD, mpierror)\n",
    "    else\n",
    "        call MPI_Bcast(args, 3, MPI_INTEGER, 0, MPI_COMM_WORLD, mpierror)\n",
    "        n = args(1); energy = args(2); niters  = args(3)\n",
    "    endif\n",
    "    \n",
    "    ! Creates a division of processors in a Cartesian grid\n",
    "    ! MPI_DIMS_CREATE(NNODES, NDIMS, DIMS, IERROR)\n",
    "    !   NNODES - number of nodes in a grid\n",
    "    !   NDIMS - number of Cartesian dimensions \n",
    "    !   DIMS - array specifying the number of nodes in each dimension\n",
    "    ! Examples:\n",
    "    !   MPI_Dims_create(6, 2, dims)  ->  (3,2)\n",
    "    !   MPI_Dims_create(7, 2, dims)  ->  (7,1)\n",
    "    call MPI_Dims_create(mpisize, 2, pdims, mpierror)\n",
    "\n",
    "    ! determine my coordinates (x,y)\n",
    "    px = pdims(1)\n",
    "    py = pdims(2)\n",
    "    rx = mod(mpirank, px)\n",
    "    ry = mpirank / px\n",
    "\n",
    "    ! determine my four neighbors\n",
    "    north = (ry - 1) * px + rx; if( (ry - 1) < 0  ) north = MPI_PROC_NULL\n",
    "    south = (ry + 1) * px + rx; if( (ry + 1) >= py) south = MPI_PROC_NULL\n",
    "    west = ry * px + rx - 1;    if( (rx - 1) < 0  ) west  = MPI_PROC_NULL\n",
    "    east = ry * px + rx + 1;    if( (rx + 1) >= px) east  = MPI_PROC_NULL\n",
    "\n",
    "    ! decompose the domain   \n",
    "    bx = n / px             ! block size in x\n",
    "    by = n / py             ! block size in y\n",
    "    offx = (rx * bx) + 1    ! offset in x\n",
    "    offy = (ry * by) + 1    ! offset in y\n",
    "\n",
    "    ! initialize heat sources\n",
    "    sources = reshape( [ n/2,   n/2,        &\n",
    "                         n/3,   n/3,        &\n",
    "                         n*4/5, n*8/9 ],    &\n",
    "              shape(sources), order=[2, 1])\n",
    "\n",
    "    do i = 1, nsources      ! determine which sources are in my patch\n",
    "        locx = sources(i, 1) - offx\n",
    "        locy = sources(i, 2) - offy    \n",
    "        if(locx >= 0 .and. locx <= bx .and. locy >= 0 .and. locy <= by) then\n",
    "            locnsources = locnsources + 1\n",
    "            locsources(locnsources, 1) = locx + 2\n",
    "            locsources(locnsources, 2) = locy + 2\n",
    "        endif\n",
    "    enddo\n",
    "\n",
    "    ! allocate communication buffers\n",
    "    allocate(sendnorthgz(bx))   ! send buffers\n",
    "    allocate(sendsouthgz(bx))\n",
    "    allocate(recvnorthgz(bx))   ! receive buffers\n",
    "    allocate(recvsouthgz(bx))\n",
    "    ! allocate two work arrays\n",
    "    allocate(aold(bx+2, by+2)); aold = 0.0   ! 1-wide halo zones!\n",
    "    allocate(anew(bx+2, by+2)); anew = 0.0   ! 1-wide halo zones!\n",
    "\n",
    "    ! laco principal das iteracoes\n",
    "    do iters = 1, niters, 2\n",
    "\n",
    "        ! --- anew <- stencil(aold) ---\n",
    "        if(north /= MPI_PROC_NULL) then \n",
    "            sendnorthgz = aold(2, 2:bx+1)\n",
    "            recvnorthgz = 0.0\n",
    "            call MPI_IRecv(recvnorthgz, bx, MPI_DOUBLE_PRECISION, north,  &\n",
    "                            mpitag, MPI_COMM_WORLD, recvrequest(1), mpierror)\n",
    "            call MPI_ISend(sendnorthgz, bx, MPI_DOUBLE_PRECISION, north,  &\n",
    "                            mpitag, MPI_COMM_WORLD, sendrequest(1), mpierror)\n",
    "        endif   \n",
    "        if(south /= MPI_PROC_NULL) then \n",
    "            sendsouthgz = aold(bx+1, 2:bx+1)\n",
    "            recvsouthgz(:) = 0.0\n",
    "            call MPI_IRecv(recvsouthgz, bx, MPI_DOUBLE_PRECISION, south,  &\n",
    "                            mpitag, MPI_COMM_WORLD, recvrequest(2), mpierror)\n",
    "            call MPI_ISend(sendsouthgz, bx, MPI_DOUBLE_PRECISION, south,  &\n",
    "                            mpitag, MPI_COMM_WORLD, sendrequest(2), mpierror)\n",
    "        endif    \n",
    "        if(east /= MPI_PROC_NULL) then \n",
    "            call MPI_IRecv(aold(2:bx+1, bx+2), bx, MPI_DOUBLE_PRECISION, east, &\n",
    "                            mpitag, MPI_COMM_WORLD, recvrequest(3), mpierror)\n",
    "            call MPI_ISend(aold(2:bx+1, bx+1), bx, MPI_DOUBLE_PRECISION, east, &\n",
    "                            mpitag, MPI_COMM_WORLD, sendrequest(3), mpierror)\n",
    "        endif    \n",
    "        if(west /= MPI_PROC_NULL) then \n",
    "            call MPI_IRecv(aold(2:bx+1, 1), bx, MPI_DOUBLE_PRECISION, west, &\n",
    "                           mpitag, MPI_COMM_WORLD, recvrequest(4), mpierror)\n",
    "            call MPI_ISend(aold(2:bx+1, 2), bx, MPI_DOUBLE_PRECISION, west, &\n",
    "                           mpitag, MPI_COMM_WORLD, sendrequest(4), mpierror)\n",
    "            endif\n",
    "        if(north /= MPI_PROC_NULL) then \n",
    "            call MPI_Wait(recvrequest(1), MPI_STATUS_IGNORE, mpierror)\n",
    "            call MPI_Wait(sendrequest(1), MPI_STATUS_IGNORE, mpierror)\n",
    "            aold(1, 2:bx+1)=recvnorthgz\n",
    "        endif\n",
    "        if(south /= MPI_PROC_NULL) then \n",
    "            call MPI_Wait(recvrequest(2), MPI_STATUS_IGNORE, mpierror)\n",
    "            call MPI_Wait(sendrequest(2), MPI_STATUS_IGNORE, mpierror)\n",
    "            aold(bx+2, 2:bx+1)=recvsouthgz\n",
    "        endif\n",
    "        if(east /= MPI_PROC_NULL) then \n",
    "            call MPI_Wait(recvrequest(3), MPI_STATUS_IGNORE, mpierror)\n",
    "            call MPI_Wait(sendrequest(3), MPI_STATUS_IGNORE, mpierror)\n",
    "        endif\n",
    "        if(west /= MPI_PROC_NULL) then \n",
    "            call MPI_Wait(recvrequest(4), MPI_STATUS_IGNORE, mpierror)\n",
    "            call MPI_Wait(sendrequest(4), MPI_STATUS_IGNORE, mpierror)\n",
    "        endif  \n",
    "\n",
    "        ! update grid points\n",
    "        do j = 2, by+1 \n",
    "            do i = 2, bx+1\n",
    "                anew(i,j)=1/2.0*(aold(i,j)+1/4.0*(aold(i-1,j)+aold(i+1,j)+aold(i,j-1)+aold(i,j+1)))\n",
    "            enddo\n",
    "        enddo\n",
    "\n",
    "        ! adiciona calor a malha\n",
    "        do i = 1, locnsources\n",
    "            anew(locsources(i, 1), locsources(i, 2)) =   &\n",
    "                anew(locsources(i, 1), locsources(i, 2)) + energy\n",
    "        enddo\n",
    "\n",
    "        ! --- aold <- stencil(anew) ---\n",
    "        if(north /= MPI_PROC_NULL) then \n",
    "            sendnorthgz=anew(2, 2:bx+1)\n",
    "            call MPI_IRecv(recvnorthgz, bx, MPI_DOUBLE_PRECISION, north, mpitag,  &\n",
    "                            MPI_COMM_WORLD, recvrequest(1), mpierror)\n",
    "            call MPI_ISend(sendnorthgz, bx, MPI_DOUBLE_PRECISION, north, mpitag,  &\n",
    "                            MPI_COMM_WORLD, sendrequest(1), mpierror)\n",
    "        endif\n",
    "        if(south /= MPI_PROC_NULL) then \n",
    "            sendsouthgz=anew(bx+1, 2:bx+1)\n",
    "            call MPI_IRecv(recvsouthgz, bx, MPI_DOUBLE_PRECISION, south, mpitag,  &\n",
    "                            MPI_COMM_WORLD, recvrequest(2), mpierror)   \n",
    "            call MPI_ISend(sendsouthgz, bx, MPI_DOUBLE_PRECISION, south, mpitag,  &\n",
    "                            MPI_COMM_WORLD, sendrequest(2), mpierror)\n",
    "        endif\n",
    "        if(east /= MPI_PROC_NULL) then \n",
    "            call MPI_IRecv(anew(2:bx+1, bx+2), bx, MPI_DOUBLE_PRECISION, east,  &\n",
    "                            mpitag, MPI_COMM_WORLD, recvrequest(3), mpierror)\n",
    "            call MPI_ISend(anew(2:bx+1, bx+1), bx, MPI_DOUBLE_PRECISION, east,  &\n",
    "                            mpitag, MPI_COMM_WORLD, sendrequest(3), mpierror)\n",
    "        endif\n",
    "        if(west /= MPI_PROC_NULL) then \n",
    "            call MPI_IRecv(anew(2:bx+1, 1), bx, MPI_DOUBLE_PRECISION, west, mpitag,  &\n",
    "                            MPI_COMM_WORLD, recvrequest(4), mpierror)\n",
    "            call MPI_ISend(anew(2:bx+1, 2), bx, MPI_DOUBLE_PRECISION, west, mpitag,  &\n",
    "                            MPI_COMM_WORLD, sendrequest(4), mpierror)\n",
    "        endif\n",
    "        if(north /= MPI_PROC_NULL) then \n",
    "            call MPI_Wait(recvrequest(1), MPI_STATUS_IGNORE, mpierror)\n",
    "            call MPI_Wait(sendrequest(1), MPI_STATUS_IGNORE, mpierror)\n",
    "            anew(1, 2:bx+1)=recvnorthgz\n",
    "        endif\n",
    "        if(south /= MPI_PROC_NULL) then \n",
    "            call MPI_Wait(recvrequest(2), MPI_STATUS_IGNORE, mpierror)\n",
    "            call MPI_Wait(sendrequest(2), MPI_STATUS_IGNORE, mpierror)\n",
    "            anew(bx+2, 2:bx+1)=recvsouthgz\n",
    "        endif\n",
    "        if(east /= MPI_PROC_NULL) then \n",
    "            call MPI_Wait(recvrequest(3), MPI_STATUS_IGNORE, mpierror)\n",
    "            call MPI_Wait(sendrequest(3), MPI_STATUS_IGNORE, mpierror)\n",
    "        endif\n",
    "        if(west /= MPI_PROC_NULL) then \n",
    "            call MPI_Wait(recvrequest(4), MPI_STATUS_IGNORE, mpierror)\n",
    "            call MPI_Wait(sendrequest(4), MPI_STATUS_IGNORE, mpierror)\n",
    "        endif\n",
    "\n",
    "        ! update grid points\n",
    "        do j = 2, by+1 \n",
    "            do i = 2, bx+1\n",
    "                aold(i,j)=1/2.0*(anew(i,j)+1/4.0*(anew(i-1,j)+anew(i+1,j)+anew(i,j-1)+anew(i,j+1)))\n",
    "            enddo\n",
    "        enddo\n",
    "\n",
    "        ! adiciona calor a malha:\n",
    "        do i = 1, locnsources\n",
    "            aold(locsources(i, 1), locsources(i, 2)) =  &\n",
    "                aold(locsources(i, 1), locsources(i, 2)) + energy\n",
    "        enddo\n",
    "\n",
    "    enddo\n",
    "   \n",
    "    ! ALL REDUCE:\n",
    "    heat = 0.0\n",
    "    do j = 2, by+1 \n",
    "        do i = 2, bx+1\n",
    "            heat = heat + aold(i, j)\n",
    "        enddo\n",
    "    enddo\n",
    "    call MPI_Allreduce(heat, rheat, 1, MPI_DOUBLE_PRECISION, MPI_SUM,  &\n",
    "                       MPI_COMM_WORLD, mpierror)\n",
    "\n",
    "    if(mpirank == 0) then\n",
    "        mpiwtime = mpiwtime + MPI_Wtime()\n",
    "        write(*, \"('Heat='     f0.2' | ')\", advance=\"no\") rheat\n",
    "        write(*, \"('Tempo='    f0.4' | ')\", advance=\"no\") mpiwtime\n",
    "        write(*, \"('MPI_Size=' i0  ' | ')\", advance=\"no\") mpisize\n",
    "        write(*, \"('MPI_Dims=('i0','i0') | ')\", advance=\"no\") pdims\n",
    "        write(*, \"('bx,by=('i0','i0')')\") bx,by\n",
    "    endif\n",
    "\n",
    "    call MPI_Finalize(mpierror)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slurm script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting stfopagnu485_16.srm\n"
     ]
    }
   ],
   "source": [
    "%%writefile stfopagnu485_16.srm\n",
    "#!/bin/bash\n",
    "#SBATCH --ntasks=16            #Total de tarefas\n",
    "#SBATCH -p cpu_small           #Fila (partition) a ser utilizada\n",
    "#SBATCH -J stcforpa            #Nome do job, 8 caracteres\n",
    "#SBATCH --time=00:02:00        #Tempo max. de execução 2 minutos\n",
    "#SBATCH --exclusive            #Utilização exclusiva dos nós\n",
    "\n",
    "echo '========================================'\n",
    "echo '- Job ID:' $SLURM_JOB_ID\n",
    "echo '- Tarefas por no:' $SLURM_NTASKS_PER_NODE\n",
    "echo '- Qtd. de nos:' $SLURM_JOB_NUM_NODES\n",
    "echo '- Tot. de tarefas:' $SLURM_NTASKS\n",
    "echo '- Nos alocados:' $SLURM_JOB_NODELIST\n",
    "echo '- diretorio onde sbatch foi chamado ($SLURM_SUBMIT_DIR):'\n",
    "echo $SLURM_SUBMIT_DIR\n",
    "cd $SLURM_SUBMIT_DIR\n",
    "nodeset -e $SLURM_JOB_NODELIST\n",
    "\n",
    "#Configura o ambiente\n",
    "echo '-- modulos ----------------------------'\n",
    "# echo 'module load gcc/7.4'\n",
    "# module load gcc/7.4\n",
    "echo 'module load openmpi/gnu/4.0.1'\n",
    "module load openmpi/gnu/4.0.1\n",
    "\n",
    "#Configura o executavel\n",
    "EXEC=\"/scratch/yyyy/xxxx/stnc/Fortran/stencil_mpi 4800 1 500\"\n",
    "\n",
    "#Dispara a execucao\n",
    "echo '-- srun -------------------------------'\n",
    "echo '$ srun --mpi=pmi2 -n' $SLURM_NTASKS $EXEC\n",
    "srun --mpi=pmi2 -n $SLURM_NTASKS $EXEC\n",
    "echo '-- FIM --------------------------------'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build using GNU 4.8.5 and OpenMPI 4.0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc (GCC) 4.8.5 20150623 (Red Hat 4.8.5-36)\n",
      "Copyright (C) 2015 Free Software Foundation, Inc.\n",
      "This is free software; see the source for copying conditions.  There is NO\n",
      "warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n",
      "\n",
      "GNU Fortran (GCC) 4.8.5 20150623 (Red Hat 4.8.5-36)\n",
      "Copyright (C) 2015 Free Software Foundation, Inc.\n",
      "\n",
      "GNU Fortran comes with NO WARRANTY, to the extent permitted by law.\n",
      "You may redistribute copies of GNU Fortran\n",
      "under the terms of the GNU General Public License.\n",
      "For more information about these matters, see the file named COPYING\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "module load openmpi/gnu/4.0.1\n",
    "gcc --version\n",
    "mpif90 --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rwxr-xr-x 1 xxxx yyyy 23K Dec  2 22:18 stencil_mpi\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "module load openmpi/gnu/4.0.1\n",
    "mpif90  -O3  -o stencil_mpi  stencil_mpi.f90\n",
    "ls -lh stencil_mpi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Heat=1500.00 | Tempo=20.6497 | MPI_Size=1 | MPI_Dims=(1,1) | bx,by=(4800,4800)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "module load openmpi/gnu/4.0.1\n",
    "mpiexec -n 1 ./stencil_mpi 4800 1 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy the executable to /scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 769K\n",
      "-rwxr-xr-x 1 xxxx yyyy 817K Nov  8 21:12 stencil_mpi\n",
      "-rwxr-xr-x 1 xxxx yyyy 765K Nov  8 18:59 stencil_seq\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "a='stencil_mpi'\n",
    "b='/stnc/Fortran'\n",
    "s='/prj/yyyy/xxxx'$b\n",
    "d='/scratch/yyyy/xxxx'$b\n",
    "#mkdir -p $d\n",
    "cp $s/$a $d\n",
    "ls -lh $d/$a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 781031\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# 1x1=1, 2x2=4, 3x3=9, 4x4=16, 6x6=36, 7x7=49, 8x8=64, 9x9=81\n",
    "#sbatch stfopagnu485_01.srm\n",
    "#sbatch stfopagnu485_04.srm\n",
    "#sbatch stfopagnu485_09.srm\n",
    "sbatch stfopagnu485_16.srm\n",
    "#sbatch stfopagnu485_36.srm\n",
    "#sbatch stfopagnu485_49.srm\n",
    "#sbatch stfopagnu485_64.srm\n",
    "#sbatch stfopagnu485_81.srm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
      "            781020 cpu_small stcforpa xxxx. PD       0:00      4 (Resources)\n",
      "            781029 cpu_small stcforpa xxxx. PD       0:00      4 (Resources)\n",
      "            781031 cpu_small stcforpa xxxx. PD       0:00      1 (Resources)\n"
     ]
    }
   ],
   "source": [
    "! squeue -n stcforpa  # verifica se já terminou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
      "            781020 cpu_small stcforpa xxxx. PD       0:00      4 (Resources)\n",
      "            781029 cpu_small stcforpa xxxx. PD       0:00      4 (Resources)\n"
     ]
    }
   ],
   "source": [
    "! squeue -n stcforpa  # verifica se já terminou"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First time take"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "- Job ID: 781013\n",
      "- Tarefas por no:\n",
      "- Qtd. de nos: 1\n",
      "- Tot. de tarefas: 1\n",
      "- Nos alocados: sdumont1429\n",
      "- diretorio onde sbatch foi chamado ($SLURM_SUBMIT_DIR):\n",
      "/prj/yyyy/xxxx/stnc/Fortran\n",
      "sdumont1429\n",
      "-- modulos ----------------------------\n",
      "module load openmpi/gnu/4.0.1\n",
      "-- srun -------------------------------\n",
      "$ srun --mpi=pmi2 -n 1 /scratch/yyyy/xxxx/stnc/Fortran/stencil_mpi 4800 1 500\n",
      "Heat=1500.00 | Tempo=21.8917 | MPI_Size=1 | MPI_Dims=(1,1) | bx,by=(4800,4800)\n",
      "-- FIM --------------------------------\n",
      "========================================\n",
      "- Job ID: 781014\n",
      "- Tarefas por no:\n",
      "- Qtd. de nos: 1\n",
      "- Tot. de tarefas: 4\n",
      "- Nos alocados: sdumont1454\n",
      "- diretorio onde sbatch foi chamado ($SLURM_SUBMIT_DIR):\n",
      "/prj/yyyy/xxxx/stnc/Fortran\n",
      "sdumont1454\n",
      "-- modulos ----------------------------\n",
      "module load openmpi/gnu/4.0.1\n",
      "-- srun -------------------------------\n",
      "$ srun --mpi=pmi2 -n 4 /scratch/yyyy/xxxx/stnc/Fortran/stencil_mpi 4800 1 500\n",
      "Heat=1500.00 | Tempo=7.3403 | MPI_Size=4 | MPI_Dims=(2,2) | bx,by=(2400,2400)\n",
      "-- FIM --------------------------------\n",
      "========================================\n",
      "- Job ID: 781015\n",
      "- Tarefas por no:\n",
      "- Qtd. de nos: 1\n",
      "- Tot. de tarefas: 9\n",
      "- Nos alocados: sdumont1464\n",
      "- diretorio onde sbatch foi chamado ($SLURM_SUBMIT_DIR):\n",
      "/prj/yyyy/xxxx/stnc/Fortran\n",
      "sdumont1464\n",
      "-- modulos ----------------------------\n",
      "module load openmpi/gnu/4.0.1\n",
      "-- srun -------------------------------\n",
      "$ srun --mpi=pmi2 -n 9 /scratch/yyyy/xxxx/stnc/Fortran/stencil_mpi 4800 1 500\n",
      "Heat=1500.00 | Tempo=6.0767 | MPI_Size=9 | MPI_Dims=(3,3) | bx,by=(1600,1600)\n",
      "-- FIM --------------------------------\n",
      "========================================\n",
      "- Job ID: 781031\n",
      "- Tarefas por no:\n",
      "- Qtd. de nos: 1\n",
      "- Tot. de tarefas: 16\n",
      "- Nos alocados: sdumont1429\n",
      "- diretorio onde sbatch foi chamado ($SLURM_SUBMIT_DIR):\n",
      "/prj/yyyy/xxxx/stnc/Fortran\n",
      "sdumont1429\n",
      "-- modulos ----------------------------\n",
      "module load openmpi/gnu/4.0.1\n",
      "-- srun -------------------------------\n",
      "$ srun --mpi=pmi2 -n 16 /scratch/yyyy/xxxx/stnc/Fortran/stencil_mpi 4800 1 500\n",
      "Heat=1500.00 | Tempo=4.7581 | MPI_Size=16 | MPI_Dims=(4,4) | bx,by=(1200,1200)\n",
      "-- FIM --------------------------------\n",
      "========================================\n",
      "- Job ID: 781017\n",
      "- Tarefas por no:\n",
      "- Qtd. de nos: 2\n",
      "- Tot. de tarefas: 36\n",
      "- Nos alocados: sdumont[1454,1464]\n",
      "- diretorio onde sbatch foi chamado ($SLURM_SUBMIT_DIR):\n",
      "/prj/yyyy/xxxx/stnc/Fortran\n",
      "sdumont1454 sdumont1464\n",
      "-- modulos ----------------------------\n",
      "module load openmpi/gnu/4.0.1\n",
      "-- srun -------------------------------\n",
      "$ srun --mpi=pmi2 -n 36 /scratch/yyyy/xxxx/stnc/Fortran/stencil_mpi 4800 1 500\n",
      "Heat=1500.00 | Tempo=2.1853 | MPI_Size=36 | MPI_Dims=(6,6) | bx,by=(800,800)\n",
      "-- FIM --------------------------------\n",
      "========================================\n",
      "- Job ID: 781018\n",
      "- Tarefas por no:\n",
      "- Qtd. de nos: 3\n",
      "- Tot. de tarefas: 49\n",
      "- Nos alocados: sdumont[1429,1454,1464]\n",
      "- diretorio onde sbatch foi chamado ($SLURM_SUBMIT_DIR):\n",
      "/prj/yyyy/xxxx/stnc/Fortran\n",
      "sdumont1429 sdumont1454 sdumont1464\n",
      "-- modulos ----------------------------\n",
      "module load openmpi/gnu/4.0.1\n",
      "-- srun -------------------------------\n",
      "$ srun --mpi=pmi2 -n 49 /scratch/yyyy/xxxx/stnc/Fortran/stencil_mpi 4800 1 500\n",
      "Heat=1500.00 | Tempo=1.5905 | MPI_Size=49 | MPI_Dims=(7,7) | bx,by=(685,685)\n",
      "-- FIM --------------------------------\n",
      "========================================\n",
      "- Job ID: 781019\n",
      "- Tarefas por no:\n",
      "- Qtd. de nos: 3\n",
      "- Tot. de tarefas: 64\n",
      "- Nos alocados: sdumont[1429,1454,1464]\n",
      "- diretorio onde sbatch foi chamado ($SLURM_SUBMIT_DIR):\n",
      "/prj/yyyy/xxxx/stnc/Fortran\n",
      "sdumont1429 sdumont1454 sdumont1464\n",
      "-- modulos ----------------------------\n",
      "module load openmpi/gnu/4.0.1\n",
      "-- srun -------------------------------\n",
      "$ srun --mpi=pmi2 -n 64 /scratch/yyyy/xxxx/stnc/Fortran/stencil_mpi 4800 1 500\n",
      "Heat=1500.00 | Tempo=1.2578 | MPI_Size=64 | MPI_Dims=(8,8) | bx,by=(600,600)\n",
      "-- FIM --------------------------------\n",
      "========================================\n",
      "- Job ID: 781020\n",
      "- Tarefas por no:\n",
      "- Qtd. de nos: 4\n",
      "- Tot. de tarefas: 81\n",
      "- Nos alocados: sdumont[1122-1123,1429,1454]\n",
      "- diretorio onde sbatch foi chamado ($SLURM_SUBMIT_DIR):\n",
      "/prj/yyyy/xxxx/stnc/Fortran\n",
      "sdumont1122 sdumont1123 sdumont1429 sdumont1454\n",
      "-- modulos ----------------------------\n",
      "module load openmpi/gnu/4.0.1\n",
      "-- srun -------------------------------\n",
      "$ srun --mpi=pmi2 -n 81 /scratch/yyyy/xxxx/stnc/Fortran/stencil_mpi 4800 1 500\n",
      "Heat=1500.00 | Tempo=2.0254 | MPI_Size=81 | MPI_Dims=(9,9) | bx,by=(533,533)\n",
      "-- FIM --------------------------------\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "d='/scratch/yyyy/xxxx/stnc/Fortran'\n",
    "cat $d/slurm-781013.out  #01\n",
    "cat $d/slurm-781014.out  #04\n",
    "cat $d/slurm-781015.out  #09\n",
    "cat $d/slurm-781031.out  #16\n",
    "cat $d/slurm-781017.out  #36\n",
    "cat $d/slurm-781018.out  #49\n",
    "cat $d/slurm-781019.out  #64\n",
    "cat $d/slurm-781020.out  #81"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second time take"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 788003\n",
      "Submitted batch job 788004\n",
      "Submitted batch job 788005\n",
      "Submitted batch job 788006\n",
      "Submitted batch job 788007\n",
      "Submitted batch job 788008\n",
      "Submitted batch job 788009\n",
      "Submitted batch job 788010\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# 1x1=1, 2x2=4, 3x3=9, 4x4=16, 6x6=36, 7x7=49, 8x8=64, 9x9=81\n",
    "sbatch stfopagnu485_01.srm\n",
    "sbatch stfopagnu485_04.srm\n",
    "sbatch stfopagnu485_09.srm\n",
    "sbatch stfopagnu485_16.srm\n",
    "sbatch stfopagnu485_36.srm\n",
    "sbatch stfopagnu485_49.srm\n",
    "sbatch stfopagnu485_64.srm\n",
    "sbatch stfopagnu485_81.srm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
      "            788003 cpu_small stcforpa xxxx. PD       0:00      1 (Priority)\n",
      "            788004 cpu_small stcforpa xxxx. PD       0:00      1 (Priority)\n",
      "            788005 cpu_small stcforpa xxxx. PD       0:00      1 (Priority)\n",
      "            788006 cpu_small stcforpa xxxx. PD       0:00      1 (Priority)\n",
      "            788007 cpu_small stcforpa xxxx. PD       0:00      2 (Priority)\n",
      "            788008 cpu_small stcforpa xxxx. PD       0:00      3 (Priority)\n",
      "            788009 cpu_small stcforpa xxxx. PD       0:00      3 (Priority)\n",
      "            788010 cpu_small stcforpa xxxx. PD       0:00      4 (Priority)\n"
     ]
    }
   ],
   "source": [
    "! squeue -n stcforpa  # verifica se já terminou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n"
     ]
    }
   ],
   "source": [
    "! squeue -n stcforpa  # verifica se já terminou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "- Job ID: 788003\n",
      "- Tarefas por no:\n",
      "- Qtd. de nos: 1\n",
      "- Tot. de tarefas: 1\n",
      "- Nos alocados: sdumont1149\n",
      "- diretorio onde sbatch foi chamado ($SLURM_SUBMIT_DIR):\n",
      "/prj/yyyy/xxxx/stnc/Fortran\n",
      "sdumont1149\n",
      "-- modulos ----------------------------\n",
      "module load openmpi/gnu/4.0.1\n",
      "-- srun -------------------------------\n",
      "$ srun --mpi=pmi2 -n 1 /scratch/yyyy/xxxx/stnc/Fortran/stencil_mpi 4800 1 500\n",
      "Heat=1500.00 | Tempo=21.9799 | MPI_Size=1 | MPI_Dims=(1,1) | bx,by=(4800,4800)\n",
      "-- FIM --------------------------------\n",
      "========================================\n",
      "- Job ID: 788004\n",
      "- Tarefas por no:\n",
      "- Qtd. de nos: 1\n",
      "- Tot. de tarefas: 4\n",
      "- Nos alocados: sdumont1149\n",
      "- diretorio onde sbatch foi chamado ($SLURM_SUBMIT_DIR):\n",
      "/prj/yyyy/xxxx/stnc/Fortran\n",
      "sdumont1149\n",
      "-- modulos ----------------------------\n",
      "module load openmpi/gnu/4.0.1\n",
      "-- srun -------------------------------\n",
      "$ srun --mpi=pmi2 -n 4 /scratch/yyyy/xxxx/stnc/Fortran/stencil_mpi 4800 1 500\n",
      "Heat=1500.00 | Tempo=7.3660 | MPI_Size=4 | MPI_Dims=(2,2) | bx,by=(2400,2400)\n",
      "-- FIM --------------------------------\n",
      "========================================\n",
      "- Job ID: 788005\n",
      "- Tarefas por no:\n",
      "- Qtd. de nos: 1\n",
      "- Tot. de tarefas: 9\n",
      "- Nos alocados: sdumont1149\n",
      "- diretorio onde sbatch foi chamado ($SLURM_SUBMIT_DIR):\n",
      "/prj/yyyy/xxxx/stnc/Fortran\n",
      "sdumont1149\n",
      "-- modulos ----------------------------\n",
      "module load openmpi/gnu/4.0.1\n",
      "-- srun -------------------------------\n",
      "$ srun --mpi=pmi2 -n 9 /scratch/yyyy/xxxx/stnc/Fortran/stencil_mpi 4800 1 500\n",
      "Heat=1500.00 | Tempo=6.1560 | MPI_Size=9 | MPI_Dims=(3,3) | bx,by=(1600,1600)\n",
      "-- FIM --------------------------------\n",
      "========================================\n",
      "- Job ID: 788006\n",
      "- Tarefas por no:\n",
      "- Qtd. de nos: 1\n",
      "- Tot. de tarefas: 16\n",
      "- Nos alocados: sdumont1149\n",
      "- diretorio onde sbatch foi chamado ($SLURM_SUBMIT_DIR):\n",
      "/prj/yyyy/xxxx/stnc/Fortran\n",
      "sdumont1149\n",
      "-- modulos ----------------------------\n",
      "module load openmpi/gnu/4.0.1\n",
      "-- srun -------------------------------\n",
      "$ srun --mpi=pmi2 -n 16 /scratch/yyyy/xxxx/stnc/Fortran/stencil_mpi 4800 1 500\n",
      "Heat=1500.00 | Tempo=4.7203 | MPI_Size=16 | MPI_Dims=(4,4) | bx,by=(1200,1200)\n",
      "-- FIM --------------------------------\n",
      "========================================\n",
      "- Job ID: 788007\n",
      "- Tarefas por no:\n",
      "- Qtd. de nos: 2\n",
      "- Tot. de tarefas: 36\n",
      "- Nos alocados: sdumont[1149,1272]\n",
      "- diretorio onde sbatch foi chamado ($SLURM_SUBMIT_DIR):\n",
      "/prj/yyyy/xxxx/stnc/Fortran\n",
      "sdumont1149 sdumont1272\n",
      "-- modulos ----------------------------\n",
      "module load openmpi/gnu/4.0.1\n",
      "-- srun -------------------------------\n",
      "$ srun --mpi=pmi2 -n 36 /scratch/yyyy/xxxx/stnc/Fortran/stencil_mpi 4800 1 500\n",
      "Heat=1500.00 | Tempo=2.1258 | MPI_Size=36 | MPI_Dims=(6,6) | bx,by=(800,800)\n",
      "-- FIM --------------------------------\n",
      "========================================\n",
      "- Job ID: 788008\n",
      "- Tarefas por no:\n",
      "- Qtd. de nos: 3\n",
      "- Tot. de tarefas: 49\n",
      "- Nos alocados: sdumont[1083,1149,1272]\n",
      "- diretorio onde sbatch foi chamado ($SLURM_SUBMIT_DIR):\n",
      "/prj/yyyy/xxxx/stnc/Fortran\n",
      "sdumont1083 sdumont1149 sdumont1272\n",
      "-- modulos ----------------------------\n",
      "module load openmpi/gnu/4.0.1\n",
      "-- srun -------------------------------\n",
      "$ srun --mpi=pmi2 -n 49 /scratch/yyyy/xxxx/stnc/Fortran/stencil_mpi 4800 1 500\n",
      "Heat=1500.00 | Tempo=1.5254 | MPI_Size=49 | MPI_Dims=(7,7) | bx,by=(685,685)\n",
      "-- FIM --------------------------------\n",
      "========================================\n",
      "- Job ID: 788009\n",
      "- Tarefas por no:\n",
      "- Qtd. de nos: 3\n",
      "- Tot. de tarefas: 64\n",
      "- Nos alocados: sdumont[1083,1149,1272]\n",
      "- diretorio onde sbatch foi chamado ($SLURM_SUBMIT_DIR):\n",
      "/prj/yyyy/xxxx/stnc/Fortran\n",
      "sdumont1083 sdumont1149 sdumont1272\n",
      "-- modulos ----------------------------\n",
      "module load openmpi/gnu/4.0.1\n",
      "-- srun -------------------------------\n",
      "$ srun --mpi=pmi2 -n 64 /scratch/yyyy/xxxx/stnc/Fortran/stencil_mpi 4800 1 500\n",
      "Heat=1500.00 | Tempo=1.2199 | MPI_Size=64 | MPI_Dims=(8,8) | bx,by=(600,600)\n",
      "-- FIM --------------------------------\n",
      "========================================\n",
      "- Job ID: 788010\n",
      "- Tarefas por no:\n",
      "- Qtd. de nos: 4\n",
      "- Tot. de tarefas: 81\n",
      "- Nos alocados: sdumont[1083,1149,1391,1410]\n",
      "- diretorio onde sbatch foi chamado ($SLURM_SUBMIT_DIR):\n",
      "/prj/yyyy/xxxx/stnc/Fortran\n",
      "sdumont1083 sdumont1149 sdumont1391 sdumont1410\n",
      "-- modulos ----------------------------\n",
      "module load openmpi/gnu/4.0.1\n",
      "-- srun -------------------------------\n",
      "$ srun --mpi=pmi2 -n 81 /scratch/yyyy/xxxx/stnc/Fortran/stencil_mpi 4800 1 500\n",
      "Heat=1500.00 | Tempo=2.0314 | MPI_Size=81 | MPI_Dims=(9,9) | bx,by=(533,533)\n",
      "-- FIM --------------------------------\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "d='/scratch/yyyy/xxxx/stnc/Fortran'\n",
    "cat $d/slurm-788003.out  #01\n",
    "cat $d/slurm-788004.out  #04\n",
    "cat $d/slurm-788005.out  #09\n",
    "cat $d/slurm-788006.out  #16\n",
    "cat $d/slurm-788007.out  #36\n",
    "cat $d/slurm-788008.out  #49\n",
    "cat $d/slurm-788009.out  #64\n",
    "cat $d/slurm-788010.out  #81"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Third time take"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 788013\n",
      "Submitted batch job 788014\n",
      "Submitted batch job 788015\n",
      "Submitted batch job 788016\n",
      "Submitted batch job 788017\n",
      "Submitted batch job 788018\n",
      "Submitted batch job 788019\n",
      "Submitted batch job 788020\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# 1x1=1, 2x2=4, 3x3=9, 4x4=16, 6x6=36, 7x7=49, 8x8=64, 9x9=81\n",
    "sbatch stfopagnu485_01.srm\n",
    "sbatch stfopagnu485_04.srm\n",
    "sbatch stfopagnu485_09.srm\n",
    "sbatch stfopagnu485_16.srm\n",
    "sbatch stfopagnu485_36.srm\n",
    "sbatch stfopagnu485_49.srm\n",
    "sbatch stfopagnu485_64.srm\n",
    "sbatch stfopagnu485_81.srm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
      "            788009 cpu_small stcforpa xxxx. PD       0:00      3 (Resources)\n",
      "            788010 cpu_small stcforpa xxxx. PD       0:00      4 (Resources)\n",
      "            788007 cpu_small stcforpa xxxx. PD       0:00      2 (Resources)\n",
      "            788008 cpu_small stcforpa xxxx. PD       0:00      3 (Resources)\n",
      "            788006 cpu_small stcforpa xxxx. PD       0:00      1 (Resources)\n",
      "            788005 cpu_small stcforpa xxxx. PD       0:00      1 (Resources)\n",
      "            788004 cpu_small stcforpa xxxx. PD       0:00      1 (Resources)\n",
      "            788003 cpu_small stcforpa xxxx. PD       0:00      1 (Resources)\n",
      "            788013 cpu_small stcforpa xxxx. PD       0:00      1 (Priority)\n",
      "            788014 cpu_small stcforpa xxxx. PD       0:00      1 (Priority)\n",
      "            788015 cpu_small stcforpa xxxx. PD       0:00      1 (Priority)\n",
      "            788016 cpu_small stcforpa xxxx. PD       0:00      1 (Priority)\n",
      "            788017 cpu_small stcforpa xxxx. PD       0:00      2 (Priority)\n",
      "            788018 cpu_small stcforpa xxxx. PD       0:00      3 (Priority)\n",
      "            788019 cpu_small stcforpa xxxx. PD       0:00      3 (Priority)\n",
      "            788020 cpu_small stcforpa xxxx. PD       0:00      4 (Priority)\n"
     ]
    }
   ],
   "source": [
    "! squeue -n stcforpa  # verifica se já terminou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n"
     ]
    }
   ],
   "source": [
    "! squeue -n stcforpa  # verifica se já terminou"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mostra os arquivos de saída GNU Fortran (GCC) 4.8.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "- Job ID: 788013\n",
      "- Tarefas por no:\n",
      "- Qtd. de nos: 1\n",
      "- Tot. de tarefas: 1\n",
      "- Nos alocados: sdumont1149\n",
      "- diretorio onde sbatch foi chamado ($SLURM_SUBMIT_DIR):\n",
      "/prj/yyyy/xxxx/stnc/Fortran\n",
      "sdumont1149\n",
      "-- modulos ----------------------------\n",
      "module load openmpi/gnu/4.0.1\n",
      "-- srun -------------------------------\n",
      "$ srun --mpi=pmi2 -n 1 /scratch/yyyy/xxxx/stnc/Fortran/stencil_mpi 4800 1 500\n",
      "Heat=1500.00 | Tempo=21.8653 | MPI_Size=1 | MPI_Dims=(1,1) | bx,by=(4800,4800)\n",
      "-- FIM --------------------------------\n",
      "========================================\n",
      "- Job ID: 788014\n",
      "- Tarefas por no:\n",
      "- Qtd. de nos: 1\n",
      "- Tot. de tarefas: 4\n",
      "- Nos alocados: sdumont1149\n",
      "- diretorio onde sbatch foi chamado ($SLURM_SUBMIT_DIR):\n",
      "/prj/yyyy/xxxx/stnc/Fortran\n",
      "sdumont1149\n",
      "-- modulos ----------------------------\n",
      "module load openmpi/gnu/4.0.1\n",
      "-- srun -------------------------------\n",
      "$ srun --mpi=pmi2 -n 4 /scratch/yyyy/xxxx/stnc/Fortran/stencil_mpi 4800 1 500\n",
      "Heat=1500.00 | Tempo=7.3280 | MPI_Size=4 | MPI_Dims=(2,2) | bx,by=(2400,2400)\n",
      "-- FIM --------------------------------\n",
      "========================================\n",
      "- Job ID: 788015\n",
      "- Tarefas por no:\n",
      "- Qtd. de nos: 1\n",
      "- Tot. de tarefas: 9\n",
      "- Nos alocados: sdumont1149\n",
      "- diretorio onde sbatch foi chamado ($SLURM_SUBMIT_DIR):\n",
      "/prj/yyyy/xxxx/stnc/Fortran\n",
      "sdumont1149\n",
      "-- modulos ----------------------------\n",
      "module load openmpi/gnu/4.0.1\n",
      "-- srun -------------------------------\n",
      "$ srun --mpi=pmi2 -n 9 /scratch/yyyy/xxxx/stnc/Fortran/stencil_mpi 4800 1 500\n",
      "Heat=1500.00 | Tempo=6.2291 | MPI_Size=9 | MPI_Dims=(3,3) | bx,by=(1600,1600)\n",
      "-- FIM --------------------------------\n",
      "========================================\n",
      "- Job ID: 788016\n",
      "- Tarefas por no:\n",
      "- Qtd. de nos: 1\n",
      "- Tot. de tarefas: 16\n",
      "- Nos alocados: sdumont1149\n",
      "- diretorio onde sbatch foi chamado ($SLURM_SUBMIT_DIR):\n",
      "/prj/yyyy/xxxx/stnc/Fortran\n",
      "sdumont1149\n",
      "-- modulos ----------------------------\n",
      "module load openmpi/gnu/4.0.1\n",
      "-- srun -------------------------------\n",
      "$ srun --mpi=pmi2 -n 16 /scratch/yyyy/xxxx/stnc/Fortran/stencil_mpi 4800 1 500\n",
      "Heat=1500.00 | Tempo=4.5761 | MPI_Size=16 | MPI_Dims=(4,4) | bx,by=(1200,1200)\n",
      "-- FIM --------------------------------\n",
      "========================================\n",
      "- Job ID: 788017\n",
      "- Tarefas por no:\n",
      "- Qtd. de nos: 2\n",
      "- Tot. de tarefas: 36\n",
      "- Nos alocados: sdumont[1083,1149]\n",
      "- diretorio onde sbatch foi chamado ($SLURM_SUBMIT_DIR):\n",
      "/prj/yyyy/xxxx/stnc/Fortran\n",
      "sdumont1083 sdumont1149\n",
      "-- modulos ----------------------------\n",
      "module load openmpi/gnu/4.0.1\n",
      "-- srun -------------------------------\n",
      "$ srun --mpi=pmi2 -n 36 /scratch/yyyy/xxxx/stnc/Fortran/stencil_mpi 4800 1 500\n",
      "Heat=1500.00 | Tempo=2.0752 | MPI_Size=36 | MPI_Dims=(6,6) | bx,by=(800,800)\n",
      "-- FIM --------------------------------\n",
      "========================================\n",
      "- Job ID: 788018\n",
      "- Tarefas por no:\n",
      "- Qtd. de nos: 3\n",
      "- Tot. de tarefas: 49\n",
      "- Nos alocados: sdumont[1083,1149,1272]\n",
      "- diretorio onde sbatch foi chamado ($SLURM_SUBMIT_DIR):\n",
      "/prj/yyyy/xxxx/stnc/Fortran\n",
      "sdumont1083 sdumont1149 sdumont1272\n",
      "-- modulos ----------------------------\n",
      "module load openmpi/gnu/4.0.1\n",
      "-- srun -------------------------------\n",
      "$ srun --mpi=pmi2 -n 49 /scratch/yyyy/xxxx/stnc/Fortran/stencil_mpi 4800 1 500\n",
      "Heat=1500.00 | Tempo=2.5394 | MPI_Size=49 | MPI_Dims=(7,7) | bx,by=(685,685)\n",
      "-- FIM --------------------------------\n",
      "========================================\n",
      "- Job ID: 788019\n",
      "- Tarefas por no:\n",
      "- Qtd. de nos: 3\n",
      "- Tot. de tarefas: 64\n",
      "- Nos alocados: sdumont[1083,1149,1272]\n",
      "- diretorio onde sbatch foi chamado ($SLURM_SUBMIT_DIR):\n",
      "/prj/yyyy/xxxx/stnc/Fortran\n",
      "sdumont1083 sdumont1149 sdumont1272\n",
      "-- modulos ----------------------------\n",
      "module load openmpi/gnu/4.0.1\n",
      "-- srun -------------------------------\n",
      "$ srun --mpi=pmi2 -n 64 /scratch/yyyy/xxxx/stnc/Fortran/stencil_mpi 4800 1 500\n",
      "Heat=1500.00 | Tempo=1.2071 | MPI_Size=64 | MPI_Dims=(8,8) | bx,by=(600,600)\n",
      "-- FIM --------------------------------\n",
      "========================================\n",
      "- Job ID: 788020\n",
      "- Tarefas por no:\n",
      "- Qtd. de nos: 4\n",
      "- Tot. de tarefas: 81\n",
      "- Nos alocados: sdumont[1083,1149,1391,1410]\n",
      "- diretorio onde sbatch foi chamado ($SLURM_SUBMIT_DIR):\n",
      "/prj/yyyy/xxxx/stnc/Fortran\n",
      "sdumont1083 sdumont1149 sdumont1391 sdumont1410\n",
      "-- modulos ----------------------------\n",
      "module load openmpi/gnu/4.0.1\n",
      "-- srun -------------------------------\n",
      "$ srun --mpi=pmi2 -n 81 /scratch/yyyy/xxxx/stnc/Fortran/stencil_mpi 4800 1 500\n",
      "Heat=1500.00 | Tempo=.9991 | MPI_Size=81 | MPI_Dims=(9,9) | bx,by=(533,533)\n",
      "-- FIM --------------------------------\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "d='/scratch/yyyy/xxxx/stnc/Fortran'\n",
    "cat $d/slurm-788013.out  #01\n",
    "cat $d/slurm-788014.out  #04\n",
    "cat $d/slurm-788015.out  #09\n",
    "cat $d/slurm-788016.out  #16\n",
    "cat $d/slurm-788017.out  #36\n",
    "cat $d/slurm-788018.out  #49\n",
    "cat $d/slurm-788019.out  #64\n",
    "cat $d/slurm-788020.out  #81"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
